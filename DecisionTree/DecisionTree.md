# 决策树
## 基本流程
- 决策树的根节点包含样本全集，内部节点包含样本子集，叶节点对应决策结果
- 决策树的训练输入包括属性集A以及训练集D，D的特征即属性的取值，D的标签即分类类别。以判断西瓜好坏为例，属性集包括色泽、根蒂、敲声、纹理、脐部、触感，D对应的一列特征为青绿、蜷缩、浊响、清晰、凹陷、硬滑，标签为好瓜（或坏瓜）
- 决策树在每个节点处，选择一个属性进行测试。根据属性测试的结果进行分叉，**可能是二叉也可能是多叉**，取决于该属性有多少个可能的取值。
- 决策树的生长是一个递归过程。有3种情形导致递归返回：当 1）当前结点包含的样本全属于同一类别，无需划分；2）当前属性值为空，或是所有样本在所有属性上取值相同，无法划分；3）当前结点包含的样本集合为空，不能划分
- 根节点包含所有属性的所有可能取值，而内部结点则未必，对内部结点的某个属性，缺失值标记为叶节点，类别取父节点所含样本最多的类别
- 在（离散）决策树生长过程中，一条路径上每个属性最多仅使用一次
- **决策树天然就支持多分类**(?)
## 划分选择
### 信息增益: ID3算法
- 采用信息熵度量样本集合纯度，信息熵越小，纯度越高
> $$Ent(D) = -\sum_{k=1}^{|\gamma|}p_k\log_2(p_k)$$
- 对某个属性，划分后各个子集信息熵加权，得到使用该属性划分后的信息熵，用原信息熵-新信息熵，即得到该属性对应的信息增益
> $$Gain(D, a) = Ent(D) - \sum_{v=1}^V {|D^v| \over |D|}Ent(D^v)$$
- 遍历所有属性，找到信息增益最大的属性，即最优划分属性
- 决策树的生长过程就是一个熵减的过程，决策树算法是一种贪心算法
- 信息增益的偏好：对可取值数目较多的属性有所偏好，如编号，由于每个样本的编号不同，编号对应的信息增益最大
- 问题：**划分后一定熵减吗？**
### 信息增益率: C4.5决策树算法
> $$Gain\_ratio(D, a) = {Gain(D, a) \over IV(a)}$$
> $$IV(a) = -\sum_{v=1}^V {|D^v| \over |D|}\log_2{|D^v| \over |D|}$$
- 信息增益率的偏好：对可取值数目较少的属性有所偏好
- C4.5采用启发式算法，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的
### 基尼指数: CART决策树算法
- 采用基尼值度量样本集合纯度，基尼值越小，纯度越高
> $$Gini(D) = 1 - \sum_{k=1}^{|\gamma|}p_k^2$$
- $Gini(D)$反映了从数据集D中随机抽取两个样本，其类别不一致的概率
- 基尼指数：划分后各个子集基尼值加权
> $$Gini\_index(D, a) = \sum_{v=1}^V {|D^v| \over |D|}Gini(D^v)$$
## 剪枝处理
- 剪枝是决策树克服过拟合的主要手段。决策树分支过多，容易导致过拟合。通过主动去掉一些分支，可降低过拟合的风险
- #TODO
## 连续与缺失值
### 连续值
- 思路：连续属性离散化
- 做法：二分法（C4.5决策树算法采用该方法）。对连续属性a，有n个取值，n-1种划分方法（候选划分点）
- 与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性
### 缺失值
- 思路：引入样本权重
- 做法（C4.5决策树算法采用该方法）：1）在选择划分属性时，信息增益乘以缺失比例；2）在对缺失样本进行划分时，将缺失样本以不同的概率划入到不同的子结点中去
## 多变量决策树
- 出发点：决策树形成的分类边界由若干个与坐标轴平行的分段组成，当学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，导致决策树相当复杂，预测时间开销很大
- 思路：斜划分，不是为每个结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器